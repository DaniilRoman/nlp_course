{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uHUidi_S6JC3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "mQYi0n328wHI",
    "outputId": "5d0e7e3e-66eb-462e-e6b2-c48dd548d7a1"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    data_prefix = '/content/drive/My Drive/NLP'\n",
    "except ModuleNotFoundError:\n",
    "    data_prefix = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "on4Ta9G67Dm2"
   },
   "outputs": [],
   "source": [
    "raw_train = pd.read_parquet(os.path.join(data_prefix, 'train.parquet'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разделение данных на тренировочную и валидационную выборки \n",
    "\n",
    "В курсе по NLP от ШАДа рекомендуют делать это разделение до начала предварительной обработки, чтобы нигде не накосячить, поэтому мы тоже так сделаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_target = raw_train['target']\n",
    "raw_data   = raw_train.drop(['target'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target = train_test_split(raw_data, raw_target,\n",
    "                                                                    test_size=0.25, random_state=47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предварительная обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/droman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from data_preprocessing import remove_html_tags, stay_only_a_z, tokenize_by_word, remove_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг для Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_preprocess(text):\n",
    "#     text = remove_html_tags(text)\n",
    "    text = stay_only_a_z(text)\n",
    "    text = tokenize_by_word(text)\n",
    "#     text = remove_stop_words(text)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выполним препроцессинг для данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-da182aa111b9>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['Body'] = train_data['Body'].apply(body_preprocess)\n",
      "<ipython-input-26-da182aa111b9>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data ['Body'] = test_data ['Body'].apply(body_preprocess)\n"
     ]
    }
   ],
   "source": [
    "train_data['Body'] = train_data['Body'].apply(body_preprocess)\n",
    "test_data ['Body'] = test_data ['Body'].apply(body_preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг для Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_preprocessing(text):\n",
    "    text = stay_only_a_z(text)\n",
    "    text = tokenize_by_word(text)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-e5c6a9bb7a00>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['Title'] = train_data['Title'].apply(title_preprocessing)\n",
      "<ipython-input-29-e5c6a9bb7a00>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data ['Title'] = test_data ['Title'].apply(title_preprocessing)\n"
     ]
    }
   ],
   "source": [
    "train_data['Title'] = train_data['Title'].apply(title_preprocessing)\n",
    "test_data ['Title'] = test_data ['Title'].apply(title_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Секция для определения исходных фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_features(__data):\n",
    "    __data['text'] = __data['Title'] + ' ' + __data['Body']\n",
    "    return __data.drop(['Tags', 'Title', 'Body'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "YlmAgFAA-jNC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-e70b16de7c75>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  __data['text'] = __data['Title'] + ' ' + __data['Body']\n"
     ]
    }
   ],
   "source": [
    "train_data = get_raw_features(train_data)\n",
    "test_data = get_raw_features(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Не забыть уйти от Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data['text']\n",
    "test_data = test_data['text']\n",
    "\n",
    "train_target = train_target.values\n",
    "test_target  = test_target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sfomX7KCdU-"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08251953, -0.03393555, -0.05200195,  0.08691406,  0.04663086,\n",
       "        0.10205078,  0.11523438,  0.14941406,  0.07617188, -0.02319336,\n",
       "       -0.28515625,  0.06298828, -0.06591797,  0.08105469, -0.18261719,\n",
       "        0.34765625,  0.25390625,  0.21386719, -0.03833008, -0.20800781,\n",
       "       -0.01513672,  0.05859375, -0.31054688,  0.3125    , -0.046875  ,\n",
       "       -0.21386719,  0.09960938, -0.140625  ,  0.13964844, -0.31445312,\n",
       "       -0.25      , -0.00430298, -0.02282715, -0.15625   , -0.19921875,\n",
       "       -0.33398438,  0.00476074, -0.20996094,  0.18164062,  0.16015625,\n",
       "       -0.11425781,  0.23535156,  0.31445312,  0.15722656,  0.01086426,\n",
       "       -0.2890625 , -0.08105469,  0.12988281,  0.0213623 , -0.0625    ,\n",
       "       -0.17871094, -0.30859375,  0.26953125, -0.16308594, -0.00300598,\n",
       "       -0.01782227, -0.05444336,  0.05200195, -0.20507812,  0.31445312,\n",
       "        0.48828125, -0.16601562, -0.23828125, -0.16992188, -0.1484375 ,\n",
       "        0.05541992, -0.19140625,  0.00189209,  0.26171875,  0.34375   ,\n",
       "       -0.1953125 ,  0.31445312,  0.05859375, -0.19238281,  0.06933594,\n",
       "       -0.23242188,  0.13867188, -0.09130859,  0.12109375,  0.09179688,\n",
       "        0.16308594, -0.02783203,  0.06176758,  0.18847656,  0.5390625 ,\n",
       "       -0.13671875, -0.20898438,  0.30859375, -0.07177734,  0.16699219,\n",
       "       -0.19238281, -0.16796875, -0.12304688, -0.38085938, -0.00418091,\n",
       "       -0.01928711,  0.14941406,  0.39648438,  0.26367188,  0.17382812,\n",
       "        0.29492188, -0.05151367, -0.18945312,  0.12792969,  0.003479  ,\n",
       "        0.16699219, -0.0300293 ,  0.29101562,  0.25390625, -0.03637695,\n",
       "       -0.02856445, -0.1328125 , -0.10107422, -0.13476562,  0.24707031,\n",
       "        0.10302734, -0.1953125 , -0.33203125,  0.08935547,  0.04150391,\n",
       "       -0.11083984, -0.328125  , -0.359375  ,  0.09521484,  0.17773438,\n",
       "        0.27148438, -0.35742188, -0.16210938, -0.08447266,  0.13378906,\n",
       "       -0.03979492,  0.04956055,  0.01074219,  0.29101562,  0.06982422,\n",
       "       -0.12353516, -0.04882812, -0.0065918 ,  0.06103516, -0.24609375,\n",
       "        0.08398438, -0.16796875, -0.15234375, -0.03735352,  0.19726562,\n",
       "       -0.0859375 , -0.01635742, -0.17480469, -0.1328125 , -0.35742188,\n",
       "        0.25      , -0.11865234,  0.27148438,  0.05615234,  0.19042969,\n",
       "       -0.49804688,  0.0480957 ,  0.15136719, -0.34765625,  0.24707031,\n",
       "       -0.16796875,  0.12255859, -0.2890625 ,  0.23242188,  0.05517578,\n",
       "       -0.11669922,  0.06835938, -0.25195312, -0.19140625,  0.02124023,\n",
       "       -0.22265625,  0.03417969,  0.24316406,  0.20117188, -0.20898438,\n",
       "       -0.07617188, -0.16113281, -0.17089844, -0.203125  ,  0.06396484,\n",
       "       -0.05541992, -0.12988281, -0.05249023, -0.14257812, -0.01239014,\n",
       "        0.0625    , -0.24316406,  0.00309753, -0.17480469,  0.02770996,\n",
       "        0.01635742, -0.12792969, -0.22851562, -0.18847656,  0.00382996,\n",
       "        0.11035156,  0.16992188, -0.0035553 , -0.03637695, -0.33984375,\n",
       "        0.16796875,  0.28125   , -0.13867188, -0.08837891, -0.02587891,\n",
       "       -0.13183594, -0.10888672, -0.42382812,  0.05029297,  0.0703125 ,\n",
       "       -0.01501465,  0.3203125 ,  0.01708984,  0.24121094, -0.38867188,\n",
       "        0.02893066,  0.04321289, -0.12597656, -0.21386719, -0.47851562,\n",
       "       -0.30664062,  0.30078125, -0.42382812, -0.62109375, -0.24804688,\n",
       "       -0.0279541 , -0.51171875, -0.03881836,  0.06542969, -0.27929688,\n",
       "       -0.13183594, -0.24804688,  0.01385498,  0.09863281,  0.390625  ,\n",
       "       -0.18261719,  0.39648438, -0.15527344,  0.03125   ,  0.14746094,\n",
       "       -0.12792969,  0.125     , -0.09228516, -0.09423828, -0.15917969,\n",
       "       -0.05932617, -0.1640625 ,  0.59375   ,  0.18847656, -0.47460938,\n",
       "        0.01184082,  0.00469971, -0.07763672,  0.06689453,  0.09667969,\n",
       "        0.21289062,  0.06835938,  0.19726562, -0.1953125 , -0.28710938,\n",
       "       -0.13671875, -0.00204468,  0.15722656,  0.01257324,  0.17285156,\n",
       "       -0.31445312, -0.01525879,  0.20019531, -0.25585938, -0.2890625 ,\n",
       "        0.24023438,  0.43359375,  0.19238281,  0.24609375,  0.10693359,\n",
       "       -0.45898438, -0.03881836,  0.06884766, -0.05029297,  0.07519531,\n",
       "        0.05151367, -0.01177979, -0.07128906,  0.12792969,  0.02880859,\n",
       "        0.16503906,  0.07617188,  0.15234375,  0.22753906,  0.07226562,\n",
       "        0.2109375 ,  0.32226562, -0.13183594,  0.08251953, -0.12792969,\n",
       "       -0.10791016,  0.10839844, -0.45507812, -0.16796875,  0.03271484],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "word2vec_model300 = api.load('word2vec-google-news-300')\n",
    "word2vec_model300['upload']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_test_str = 'algorithmm input graph g output set of msts t begin tnull egedges for all vertices in g create a tree t having single vertex b add t to t end for repeat find an edge e e having minimum weight such that one end belongs to t t and the other end does not belongs to any of the trees in t add e to t until e null im stuck on the logic for the highlighted block ive used simple objects for vertexedge and tree and for their sets used array of objects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_str_representation_mean(text, model):\n",
    "    words = text.split()\n",
    "    words_representation = [model[word] for word in words if word in model.vocab]\n",
    "    return np.mean(words_representation, axis=0)\n",
    "\n",
    "def get_w2v_str_representation_sum(text, model):\n",
    "    words = text.split()\n",
    "    words_representation = [model[word] for word in words if word in model.vocab]\n",
    "    return np.sum(words_representation, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl https://zenodo.org/record/1199620/files/SO_vectors_200.bin?download=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "so_w2v = KeyedVectors.load_word2vec_format(\"SO_vectors_200.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.154484,  12.033579, -41.343773,  38.206593, -77.40206 ,\n",
       "        46.77203 ,  28.464117,  15.215707, -20.109539,  -9.964755],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_w2v_str_representation_sum(w2v_test_str, so_w2v)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_vectorizer_sum_news(train, test):\n",
    "    return ([get_w2v_str_representation_sum(text, word2vec_model300) for text in train],\n",
    "            [get_w2v_str_representation_sum(text, word2vec_model300) for text in test])\n",
    "\n",
    "def w2v_vectorizer_mean_news(train, test):\n",
    "    return ([get_w2v_str_representation_mean(text, word2vec_model300) for text in train],\n",
    "            [get_w2v_str_representation_mean(text, word2vec_model300) for text in test])\n",
    "\n",
    "def w2v_vectorizer_sum_so(train, test):\n",
    "    return ([get_w2v_str_representation_sum(text, so_w2v) for text in train],\n",
    "            [get_w2v_str_representation_sum(text, so_w2v) for text in test])\n",
    "\n",
    "def w2v_vectorizer_mean_so(train, test):\n",
    "    return ([get_w2v_str_representation_mean(text, so_w2v) for text in train],\n",
    "            [get_w2v_str_representation_mean(text, so_w2v) for text in test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"./d2v/doc2vec.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "\n",
    "def d2v_vectorizer(train, test):\n",
    "    return ([model.infer_vector([text]) for text in train],\n",
    "            [model.infer_vector([text]) for text in test])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp8HDPeKJadX"
   },
   "source": [
    "# Построение классификаторов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTCHZxyhKhIj"
   },
   "source": [
    "## 3. Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "phyDRDQbKgpe"
   },
   "outputs": [],
   "source": [
    "def lr_score(trainX, trainY, testX, testY):\n",
    "    lr_classifier = LogisticRegression()\n",
    "    lr_classifier.fit(trainX, trainY)\n",
    "    return lr_classifier.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "803CUXuNlpoR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'========================================================================================================================\\n========================================================================================================================\\n========================================================================================================================\\n'"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''========================================================================================================================\n",
    "========================================================================================================================\n",
    "========================================================================================================================\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "XTUyvezPlu3r",
    "outputId": "b2ae5aae-0dc9-4358-cbf8-6a505610feab"
   },
   "outputs": [],
   "source": [
    "def visualize_results(score):\n",
    "    print(score)\n",
    "\n",
    "def base_pipeline(train_x, train_y, test_x, test_y,\n",
    "                  vectorizer, scorer):\n",
    "    vectorized_train_x, vectorized_test_x = vectorizer(train_x, test_x)\n",
    "    score = scorer(vectorized_train_x, train_y, vectorized_test_x, test_y)\n",
    "    visualize_results(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "tQ1u7-AQput5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8834166666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agladyshev/anaconda3/envs/unn-nlp/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "base_pipeline(train_data, train_target, test_data, test_target,\n",
    "              bow, lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "base_pipeline(train_data, train_target, test_data, test_target,\n",
    "              tf_idf, lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7144166666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "base_pipeline(train_data, train_target, test_data, test_target,\n",
    "              w2v_vectorizer_sum_so, lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7126666666666667\n"
     ]
    }
   ],
   "source": [
    "base_pipeline(train_data, train_target, test_data, test_target,\n",
    "              w2v_vectorizer_mean_news, lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/droman/Documents/diploma/venv/lib/python3.8/site-packages/sklearn/linear_model/logistic.py:430: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  warnings.warn(\"Default solver will be changed to 'lbfgs' in 0.22. \"\n",
      "/home/droman/Documents/diploma/venv/lib/python3.8/site-packages/sklearn/linear_model/logistic.py:467: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  warnings.warn(\"Default multi_class will be changed to 'auto' in\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3278333333333333\n"
     ]
    }
   ],
   "source": [
    "base_pipeline(train_data, train_target, test_data, test_target,\n",
    "              d2v_vectorizer, lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}