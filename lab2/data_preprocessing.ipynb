{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycountry in /home/droman/Documents/stuff/dl_course/venv/lib/python3.8/site-packages (20.7.3)\r\n",
      "\u001B[33mWARNING: You are using pip version 20.2.3; however, version 20.3.1 is available.\r\n",
      "You should consider upgrading via the '/home/droman/Documents/stuff/dl_course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: langdetect in /home/droman/Documents/stuff/dl_course/venv/lib/python3.8/site-packages (1.0.8)\r\n",
      "Requirement already satisfied: six in /home/droman/Documents/stuff/dl_course/venv/lib/python3.8/site-packages (from langdetect) (1.15.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 20.2.3; however, version 20.3.1 is available.\r\n",
      "You should consider upgrading via the '/home/droman/Documents/stuff/dl_course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: transliterate in /home/droman/Documents/stuff/dl_course/venv/lib/python3.8/site-packages (1.10.2)\r\n",
      "Requirement already satisfied: six>=1.1.0 in /home/droman/Documents/stuff/dl_course/venv/lib/python3.8/site-packages (from transliterate) (1.15.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 20.2.3; however, version 20.3.1 is available.\r\n",
      "You should consider upgrading via the '/home/droman/Documents/stuff/dl_course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: textdistance in /home/droman/Documents/stuff/dl_course/venv/lib/python3.8/site-packages (4.2.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 20.2.3; however, version 20.3.1 is available.\r\n",
      "You should consider upgrading via the '/home/droman/Documents/stuff/dl_course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pycountry\n",
    "!pip install langdetect\n",
    "!pip install transliterate\n",
    "!pip install textdistance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/droman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/droman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/droman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/droman/Documents/stuff/dl_course/venv/lib/python3.8/site-packages/tqdm/std.py:670: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "%pylab inline\n",
    "plt.style.use(\"bmh\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import  Pool\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from functools import reduce\n",
    "\n",
    "import pycountry\n",
    "import re\n",
    "\n",
    "import langdetect\n",
    "from transliterate import translit, get_available_language_codes, slugify\n",
    "\n",
    "import textdistance as dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_popular_words(data):\n",
    "    documents = pd.concat([data['name_1'], data['name_2']]).values.astype('U')\n",
    "    count_vectorizer = CountVectorizer(max_features=1000) # memory limit: ngram_range=(2, 5),\n",
    "    values = count_vectorizer.fit_transform(documents)\n",
    "    frequency = values.toarray().sum(axis=0)\n",
    "\n",
    "    feature_names = count_vectorizer.get_feature_names()\n",
    "    count_data = pd.DataFrame({'feature_names': feature_names, 'frequency': frequency})\n",
    "\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000) # memory limit: ngram_range=(2, 5),\n",
    "    values = tfidf_vectorizer.fit_transform(documents)\n",
    "    frequency = values.toarray().sum(axis=0)\n",
    "\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    tf_idf_data = pd.DataFrame({'feature_names': feature_names, 'frequency': frequency})\n",
    "\n",
    "\n",
    "    tf_idf_data = tf_idf_data.sort_values(by=['frequency'], ascending=False)\n",
    "    count_data = count_data.sort_values(by=['frequency'], ascending=False)\n",
    "\n",
    "    perc = 0.2\n",
    "    size = int(len(count_data) * perc)\n",
    "\n",
    "    count_stop = count_data['feature_names'].iloc[:size].values\n",
    "    tf_idf_stop = tf_idf_data['feature_names'].iloc[:size].values\n",
    "\n",
    "    popular_words = set(count_stop) | set(tf_idf_stop)\n",
    "    return popular_words\n",
    "\n",
    "def transliterate_text(text):\n",
    "    try:\n",
    "        t = slugify(text)\n",
    "    except:\n",
    "        return text\n",
    "    return t if t is not None else text\n",
    "\n",
    "def clear_data(data, is_train=True):\n",
    "    # cities = pd.read_csv(\"../cities.csv\", header=None, encoding='iso-8859-1')\n",
    "\n",
    "    # drop incorrect empty values\n",
    "    if is_train:\n",
    "        data = data.drop_duplicates()\n",
    "        data = data.dropna()\n",
    "        data = data[data['name_1'].astype(bool) & data['name_2'].astype(bool)]\n",
    "\n",
    "    data['name_1'] = data['name_1'].apply(transliterate_text)\n",
    "    data['name_2'] = data['name_2'].apply(transliterate_text)\n",
    "\n",
    "    countries = [country.name.lower() for country in pycountry.countries]\n",
    "\n",
    "    data[\"name_1\"] = data[\"name_1\"].str.lower()\n",
    "    data[\"name_2\"] = data[\"name_2\"].str.lower()\n",
    "\n",
    "    # REMOVE COMPANIES LABELS\n",
    "    legal_entities = [\"ltd.\", \"co.\", \"inc.\", \"b.v.\", \"s.c.r.l.\", \"gmbh\", \"pvt.\"]\n",
    "\n",
    "    for entity in tqdm(legal_entities):\n",
    "        data.replace(re.compile(f\"\\s+{entity}\\s*\"), \"\", inplace=True)\n",
    "\n",
    "    # REMOVE DATA IN CORNERS\n",
    "    data.replace(re.compile(r\"\\s+\\(.*\\)\"), \"\", inplace=True)\n",
    "    data.replace(re.compile(r\"[^\\w\\s]\"), \"\", inplace=True)\n",
    "\n",
    "    # REMOVE COUNTRY WORDS\n",
    "    for country in tqdm(countries):\n",
    "        data.replace(re.compile(country), \"\", inplace=True)\n",
    "\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "\n",
    "    print(\"Start remmoving stop words\")\n",
    "    data['name_1'] = data['name_1'].apply(lambda sentence: ' '.join([w for w in sentence.rstrip().split() if not w in stop_words]))\n",
    "    data['name_2'] = data['name_2'].apply(lambda sentence: ' '.join([w for w in sentence.rstrip().split() if not w in stop_words]))\n",
    "\n",
    "    # LEMMATIZE\n",
    "    # приводит к нормальной форме\n",
    "    lemmatizer = WordNetLemmatizer() #  lemmatizer.lemmatize(word)\n",
    "    # отрезает окончание\n",
    "    stemmer = PorterStemmer() # stemmer.stem(word)\n",
    "\n",
    "    print(\"Start lemmatization\")\n",
    "    data['name_1'] = data['name_1'].apply(lambda sentence: ' '.join([lemmatizer.lemmatize(w) for w in sentence.rstrip().split()]))\n",
    "    data['name_2'] = data['name_2'].apply(lambda sentence: ' '.join([lemmatizer.lemmatize(w) for w in sentence.rstrip().split()]))\n",
    "\n",
    "    data.fillna('', inplace=True)\n",
    "    popular_words = get_popular_words(data)\n",
    "\n",
    "    print(\"Start removing popular words\")\n",
    "    data['name_1'] = data['name_1'].apply(lambda s: ' '.join([w for w in s.rstrip().split() if not w in popular_words]))\n",
    "    data['name_2'] = data['name_2'].apply(lambda s: ' '.join([w for w in s.rstrip().split() if not w in popular_words]))\n",
    "\n",
    "    # drop incorrect empty values\n",
    "    if is_train:\n",
    "        data = data.drop_duplicates()\n",
    "        data = data.dropna()\n",
    "        data = data[data['name_1'].astype(bool) & data['name_2'].astype(bool)]\n",
    "    return data\n",
    "\n",
    "LIST_UNIQUE_COMPANIES_ID = 0\n",
    "DICT_OF_RELATIONS_ID = 1\n",
    "COUNT_OF_RELATIONS_ID = 2\n",
    "EXTRA_RELATIONS_ID = 3\n",
    "PERCENT_OF_COMPANIES_ID = 4\n",
    "\n",
    "def get_same_companies_graph(companies_dataframe):\n",
    "    result = [] # [[[a,b,c], {a: [b, c], b: [c]}, 3], [[d,e], {d: [e]}], 2]\n",
    "    for i, pair in tqdm(companies_dataframe.iterrows()):\n",
    "        first = pair['name_1']\n",
    "        second = pair['name_2']\n",
    "        if first != second:\n",
    "            flag = True\n",
    "\n",
    "            for list_same_companies in result:\n",
    "                if first in list_same_companies[0]:\n",
    "                    if second not in list_same_companies[0]:\n",
    "                        list_same_companies[0].append(second)\n",
    "                        try:\n",
    "                            list_same_companies[1][first].append(second)\n",
    "                        except:\n",
    "                            list_same_companies[1].update({first: [second]})\n",
    "                        list_same_companies[2]+=1\n",
    "                        flag = False\n",
    "                else:\n",
    "                    if second in list_same_companies[0]:\n",
    "                        if first not in list_same_companies[0]:\n",
    "                            list_same_companies[0].append(first)\n",
    "                            try:\n",
    "                                list_same_companies[1][second].append(first)\n",
    "                            except:\n",
    "                                list_same_companies[1].update({second: [first]})\n",
    "                            list_same_companies[2]+=1\n",
    "                            flag = False\n",
    "            if flag:\n",
    "                result.append([[first, second], {first: [second]}, 1])\n",
    "    return result\n",
    "\n",
    "def get_extra_relations(data):\n",
    "    data = data[data.is_duplicate==1]\n",
    "    data = data[['name_1','name_2']]\n",
    "\n",
    "    companies_graph = get_same_companies_graph(data)\n",
    "\n",
    "    def get_extra_relations(inner_companies_graph):\n",
    "        extra_relations = []\n",
    "        for unique_companies in tqdm(inner_companies_graph):\n",
    "            unique_companies.append([])\n",
    "            companies = unique_companies[LIST_UNIQUE_COMPANIES_ID]\n",
    "            company_relations = unique_companies[DICT_OF_RELATIONS_ID]\n",
    "            for company in companies:\n",
    "                try:\n",
    "                    relations = company_relations[company]\n",
    "                    excluded_companies = relations\n",
    "                except:\n",
    "                    excluded_companies = []\n",
    "                excluded_companies.append(company)\n",
    "                for extra_relation_company in companies:\n",
    "                    if extra_relation_company not in excluded_companies:\n",
    "                        extra_relations.append([company, extra_relation_company])\n",
    "                        unique_companies[COUNT_OF_RELATIONS_ID]+=1\n",
    "                        unique_companies[EXTRA_RELATIONS_ID].append([company, extra_relation_company])\n",
    "        return extra_relations\n",
    "\n",
    "    extra_relations = get_extra_relations(companies_graph)\n",
    "    df = pd.DataFrame(extra_relations, columns = ['name_1','name_2'])\n",
    "    df['is_duplicate'] =   1\n",
    "    return df, companies_graph\n",
    "\n",
    "def get_abbreviation(data):\n",
    "    data = data[data.is_duplicate==0]\n",
    "\n",
    "    data = data['name_1'].append(data['name_2']).reset_index(drop=True)\n",
    "    def filter_condition(sentence):\n",
    "        words = sentence.split()\n",
    "        return 2 < len(words) < 6 and all(len(word)>3 for word in words)\n",
    "\n",
    "    data = list(filter(filter_condition, data.tolist()))\n",
    "\n",
    "    def _get_abbreviation(sentence):\n",
    "        return ''.join([word[0] for word in sentence.split()])\n",
    "    data = [[sentence, _get_abbreviation(sentence)] for sentence in data]\n",
    "\n",
    "    return pd.DataFrame(data, columns=['name_1', 'name_2'])\n",
    "\n",
    "\n",
    "def normalize(values):\n",
    "    return (values - values.min()) / (values.max() - values.min())\n",
    "\n",
    "def calculate_distance(df, col_1=\"name_1\", col_2=\"name_2\", prefix=\"\"):\n",
    "    print(\"First part of distance finished.\")\n",
    "    df[prefix + \"jaccard\"] = df.apply(lambda r: dist.jaccard(r[col_1], r[col_2]), axis=1)\n",
    "    df[prefix + \"cosine\"]  = df.apply(lambda r: dist.cosine(r[col_1], r[col_2]), axis=1)\n",
    "    df[prefix + \"mra\"]     = df.apply(lambda r: dist.mra(r[col_1], r[col_2]), axis=1)\n",
    "\n",
    "    print(\"Second part of distance finished.\")\n",
    "    dist.jaro_winkler.external = False\n",
    "    df[prefix + \"jaro_winkler\"] = df.apply(lambda r: dist.jaro_winkler(r[col_1], r[col_2]), axis=1)\n",
    "\n",
    "    df[prefix + \"hamming\"] = df.apply(lambda r: dist.hamming(r[col_1], r[col_2]), axis=1)\n",
    "    df[prefix + \"mlipns\"]  = df.apply(lambda r: dist.mlipns(r[col_1], r[col_2]), axis=1)\n",
    "    df[prefix + \"damerau_levenshtein\"] = df.apply(lambda r: dist.damerau_levenshtein(r[col_1], r[col_2]), axis=1)\n",
    "    df[prefix + \"strcmp95\"] = df.apply(lambda r: dist.strcmp95(r[col_1], r[col_2]), axis=1)\n",
    "    df[prefix + \"needleman_wunsch\"] = df.apply(lambda r: dist.needleman_wunsch(r[col_1], r[col_2]), axis=1)\n",
    "\n",
    "    print(\"Third part of distance finished.\")\n",
    "    # error # df[prefix + \"gotoh\"] = df.apply(lambda r: dist.gotoh(r[col_1], r[col_2]), axis=1)\n",
    "    df[prefix + \"sorensen\"] = df.apply(lambda r: dist.sorensen(r[col_1], r[col_2]), axis=1)\n",
    "    df[prefix + \"tversky\"]  = df.apply(lambda r: dist.tversky(r[col_1], r[col_2]), axis=1)\n",
    "    df[prefix + \"overlap\"]  = df.apply(lambda r: dist.overlap(r[col_1], r[col_2]), axis=1)\n",
    "    #  many inf #df[prefix + \"tanimoto\"] = df.apply(lambda r: dist.tanimoto(r[col_1], r[col_2]), axis=1)\n",
    "    df[prefix + \"monge_elkan\"] = df.apply(lambda r: dist.monge_elkan(r[col_1], r[col_2]), axis=1)\n",
    "\n",
    "    print(\"Fourth part of distance finished.\")\n",
    "    df[prefix + \"bag\"]    = df.apply(lambda r: dist.bag(r[col_1], r[col_2]), axis=1)\n",
    "    df[prefix + \"lcsseq\"] = df.apply(lambda r: dist.lcsseq(r[col_1], r[col_2]), axis=1)\n",
    "    df[prefix + \"lcsstr\"] = df.apply(lambda r: dist.lcsstr(r[col_1], r[col_2]), axis=1)\n",
    "    df[prefix + \"ratcliff_obershelp\"] = df.apply(lambda r: dist.ratcliff_obershelp(r[col_1], r[col_2]), axis=1)\n",
    "    return df\n",
    "\n",
    "def get_split_index(inner_companies_graph, input_percent=20):\n",
    "    percent_count = 0\n",
    "    for i in range(len(inner_companies_graph)-1, -1, -1):\n",
    "        if percent_count >= input_percent:\n",
    "            return i\n",
    "        percent_count+=inner_companies_graph[i][PERCENT_OF_COMPANIES_ID]\n",
    "\n",
    "def get_splited_data(companies_graph, index):\n",
    "    train_graph = companies_graph[:index]\n",
    "    val_graph = companies_graph[index:]\n",
    "    def flat_graph_companies(graph_data):\n",
    "        def flat_one_company_dict(company_dict):\n",
    "            key = list(company_dict.items())[0][0]\n",
    "            values = list(company_dict.items())[0][1]\n",
    "            result = []\n",
    "            for company in values:\n",
    "                result.append([key, company])\n",
    "            return result\n",
    "\n",
    "        graph_list = list(map(lambda x: flat_one_company_dict(x[DICT_OF_RELATIONS_ID]), graph_data))\n",
    "        return [item for sublist in graph_list for item in sublist]\n",
    "    train_data = flat_graph_companies(train_graph)\n",
    "    val_data = flat_graph_companies(val_graph)\n",
    "    return train_data, val_data\n",
    "\n",
    "def split_duplicate_data(companies_graph, percent=20):\n",
    "    companies_graph.sort(key=lambda x: x[COUNT_OF_RELATIONS_ID])\n",
    "    total_companies_number = reduce(lambda x1, x2: x1+x2, map(lambda x: x[COUNT_OF_RELATIONS_ID], companies_graph))\n",
    "    for company in companies_graph:\n",
    "        company.append(100*(company[COUNT_OF_RELATIONS_ID]/total_companies_number))\n",
    "    split_index = get_split_index(companies_graph, percent)\n",
    "    train_data, val_data = get_splited_data(companies_graph, split_index)\n",
    "    train_data = pd.DataFrame(train_data, columns=[\"name_1\", \"name_2\"])\n",
    "    val_data = pd.DataFrame(val_data, columns=[\"name_1\", \"name_2\"])\n",
    "    train_data[\"is_duplicate\"] = 1\n",
    "    val_data[\"is_duplicate\"] = 1\n",
    "    return train_data, val_data\n",
    "\n",
    "def split_not_duplicate_data(data, percent=20):\n",
    "    duplicate_data = data[data[\"is_duplicate\"]==0]\n",
    "    data_train, data_val = train_test_split(duplicate_data, test_size=percent, random_state=42)\n",
    "    return data_train, data_val\n",
    "\n",
    "def parallelize_dataframe(df, func, n_cores=20):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def parallel_clear_data(data):\n",
    "    return parallelize_dataframe(data, clear_data)\n",
    "\n",
    "\n",
    "def parallel_calculate_distance(data):\n",
    "    return parallelize_dataframe(data, calculate_distance)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "wiki_data = pd.read_csv(\"./data/cleared_wiki_data.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497819\n",
      "2960\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(wiki_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "wiki_data['is_duplicate'] = 1\n",
    "train = train.drop(['pair_id'], axis=1)\n",
    "train = pd.concat([train, wiki_data])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500779\n"
     ]
    },
    {
     "data": {
      "text/plain": "                              name_1                                name_2  \\\n0                Iko Industries Ltd.  Enormous Industrial Trade Pvt., Ltd.   \n1            Apcotex Industries Ltd.   Technocraft Industries (India) Ltd.   \n2  Rishichem Distributors Pvt., Ltd.                                   Dsa   \n3            Powermax Rubber Factory                               Co. One   \n4                          Tress A/S      Longyou Industries Park Zhejiang   \n\n   is_duplicate  \n0             0  \n1             0  \n2             0  \n3             0  \n4             0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name_1</th>\n      <th>name_2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Iko Industries Ltd.</td>\n      <td>Enormous Industrial Trade Pvt., Ltd.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Apcotex Industries Ltd.</td>\n      <td>Technocraft Industries (India) Ltd.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Rishichem Distributors Pvt., Ltd.</td>\n      <td>Dsa</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Powermax Rubber Factory</td>\n      <td>Co. One</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Tress A/S</td>\n      <td>Longyou Industries Park Zhejiang</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train))\n",
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  2.18it/s]\n",
      " 57%|█████▋    | 4/7 [00:01<00:01,  2.07it/s]\n",
      "100%|██████████| 7/7 [00:03<00:00,  1.73it/s]\n",
      " 57%|█████▋    | 4/7 [00:02<00:01,  1.67it/s]\n",
      "100%|██████████| 7/7 [00:03<00:00,  1.79it/s]\n",
      "100%|██████████| 7/7 [00:03<00:00,  1.80it/s]\n",
      " 86%|████████▌ | 6/7 [00:03<00:00,  1.62it/s]\n",
      "100%|██████████| 7/7 [00:03<00:00,  2.02it/s]\n",
      " 71%|███████▏  | 5/7 [00:03<00:01,  1.68it/s]\n",
      " 86%|████████▌ | 6/7 [00:03<00:00,  1.91it/s]\n",
      "  0%|          | 1/249 [00:00<01:09,  3.59it/s]\n",
      "  0%|          | 1/249 [00:00<01:17,  3.19it/s]\n",
      "100%|██████████| 7/7 [00:04<00:00,  1.73it/s]\n",
      "\n",
      "  0%|          | 0/249 [00:00<?, ?it/s].90it/s]\n",
      "100%|██████████| 7/7 [00:03<00:00,  1.79it/s]s]\n",
      "  1%|          | 3/249 [00:01<01:22,  2.98it/s]\n",
      "100%|██████████| 7/7 [00:04<00:00,  1.75it/s]s]\n",
      "100%|██████████| 7/7 [00:03<00:00,  1.77it/s]s]\n",
      "  1%|          | 2/249 [00:00<01:33,  2.64it/s]\n",
      " 96%|█████████▋| 240/249 [01:28<00:02,  3.41it/s]\n",
      " 97%|█████████▋| 241/249 [01:27<00:03,  2.63it/s]\n",
      " 96%|█████████▌| 239/249 [01:28<00:03,  2.71it/s]\n",
      " 98%|█████████▊| 244/249 [01:29<00:01,  2.89it/s]\n",
      " 98%|█████████▊| 243/249 [01:29<00:02,  2.91it/s]\n",
      "100%|██████████| 249/249 [01:30<00:00,  2.74it/s]\n",
      " 98%|█████████▊| 244/249 [01:30<00:01,  3.17it/s]\n",
      " 97%|█████████▋| 242/249 [01:30<00:02,  2.61it/s]\n",
      " 99%|█████████▉| 247/249 [01:31<00:00,  2.76it/s]\n",
      "100%|██████████| 249/249 [01:31<00:00,  2.58it/s]\n",
      "100%|██████████| 249/249 [01:30<00:00,  2.75it/s]\n",
      "100%|██████████| 249/249 [01:30<00:00,  2.77it/s]\n",
      "100%|██████████| 249/249 [01:30<00:00,  2.74it/s]\n",
      "100%|██████████| 249/249 [01:32<00:00,  2.70it/s]\n",
      "100%|██████████| 249/249 [01:31<00:00,  2.71it/s]\n",
      "\n",
      " 99%|█████████▉| 246/249 [01:31<00:01,  2.49it/s]\n",
      "100%|██████████| 249/249 [01:32<00:00,  2.71it/s]\n",
      "100%|██████████| 249/249 [01:33<00:00,  2.68it/s]\n",
      "100%|██████████| 249/249 [01:33<00:00,  2.66it/s]\n",
      "4744it [00:01, 4380.81it/s]\n",
      "100%|██████████| 1213/1213 [00:00<00:00, 4152.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start remmoving stop words\n",
      "Start remmoving stop words\n",
      "Start remmoving stop words\n",
      "Start remmoving stop words\n",
      "Start remmoving stop words\n",
      "Start remmoving stop words\n",
      "Start remmoving stop words\n",
      "Start remmoving stop words\n",
      "Start remmoving stop wordsStart lemmatization\n",
      "\n",
      "Start lemmatization\n",
      "Start lemmatization\n",
      "Start remmoving stop words\n",
      "Start remmoving stop wordsStart remmoving stop words\n",
      "Start remmoving stop words\n",
      "\n",
      "Start remmoving stop words\n",
      "Start remmoving stop wordsStart remmoving stop words\n",
      "\n",
      "Start lemmatizationStart lemmatizationStart lemmatization\n",
      "\n",
      "\n",
      "Start remmoving stop words\n",
      "Start lemmatization\n",
      "Start remmoving stop words\n",
      "Start lemmatization\n",
      "Start remmoving stop words\n",
      "Start lemmatization\n",
      "Start remmoving stop words\n",
      "Start lemmatization\n",
      "Start lemmatization\n",
      "Start lemmatization\n",
      "Start lemmatization\n",
      "Start lemmatization\n",
      "Start lemmatization\n",
      "Start lemmatization\n",
      "Start lemmatization\n",
      "Start lemmatization\n",
      "Start lemmatization\n",
      "Start lemmatization\n",
      "Start removing popular words\n",
      "Start removing popular words\n",
      "Start removing popular words\n",
      "Start removing popular wordsStart removing popular words\n",
      "\n",
      "Start removing popular words\n",
      "Start removing popular words\n",
      "Start removing popular words\n",
      "Start removing popular wordsStart removing popular words\n",
      "\n",
      "Start removing popular words\n",
      "Start removing popular words\n",
      "Start removing popular words\n",
      "Start removing popular words\n",
      "Start removing popular words\n",
      "Start removing popular words\n",
      "Start removing popular words\n",
      "Start removing popular words\n",
      "Start removing popular words\n",
      "Start removing popular words\n",
      "Clearing finished\n",
      "Getting extra data finished\n",
      "Spliting data finished\n",
      "First part of distance finished.\n",
      "First part of distance finished.\n",
      "First part of distance finished.\n",
      "First part of distance finished.\n",
      "First part of distance finished.First part of distance finished.\n",
      "\n",
      "First part of distance finished.First part of distance finished.\n",
      "\n",
      "First part of distance finished.First part of distance finished.\n",
      "First part of distance finished.\n",
      "\n",
      "First part of distance finished.\n",
      "First part of distance finished.First part of distance finished.\n",
      "First part of distance finished.\n",
      "\n",
      "First part of distance finished.\n",
      "First part of distance finished.\n",
      "First part of distance finished.\n",
      "First part of distance finished.\n",
      "First part of distance finished.\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.Second part of distance finished.\n",
      "\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.Second part of distance finished.\n",
      "\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.\n",
      "Second part of distance finished.\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.Third part of distance finished.\n",
      "\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.Third part of distance finished.\n",
      "\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.\n",
      "Third part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "Fourth part of distance finished.\n",
      "First part of distance finished.\n",
      "First part of distance finished.First part of distance finished.First part of distance finished.First part of distance finished.First part of distance finished.First part of distance finished.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "First part of distance finished.First part of distance finished.First part of distance finished.First part of distance finished.First part of distance finished.Second part of distance finished.\n",
      "\n",
      "First part of distance finished.\n",
      "\n",
      "\n",
      "First part of distance finished.\n",
      "First part of distance finished.First part of distance finished.\n",
      "Second part of distance finished.First part of distance finished.Second part of distance finished.Second part of distance finished.First part of distance finished."
     ]
    }
   ],
   "source": [
    "data = parallel_clear_data(train)\n",
    "data.to_csv(\"data/clear_data.csv\")\n",
    "print(\"Clearing finished\")\n",
    "\n",
    "extra_relations, companies_graph = get_extra_relations(data)\n",
    "extra_relations['is_duplicate'] = 1\n",
    "\n",
    "abbreviations = get_abbreviation(data)\n",
    "abbreviations['is_duplicate'] = 1\n",
    "\n",
    "data_tmp = pd.concat([data, extra_relations, abbreviations])\n",
    "data_tmp.to_csv(\"data/extra_clear_data.csv\")\n",
    "print(\"Getting extra data finished\")\n",
    "\n",
    "data = pd.concat([data, abbreviations])\n",
    "\n",
    "train_duplicate_data, val_duplicate_data = split_duplicate_data(companies_graph, 20)\n",
    "train_not_duplicate_data, val_not_duplicate_data = split_not_duplicate_data(data, 20)\n",
    "\n",
    "train = pd.concat([train_duplicate_data, train_not_duplicate_data])\n",
    "val = pd.concat([val_duplicate_data, val_not_duplicate_data])\n",
    "\n",
    "\n",
    "train.to_csv(\"data/train_split_data.csv\")\n",
    "val.to_csv(\"data/val_split_data.csv\")\n",
    "print(\"Spliting data finished\")\n",
    "\n",
    "train_with_distance = parallel_calculate_distance(train)\n",
    "val_with_distance = parallel_calculate_distance(val)\n",
    "\n",
    "train_with_distance.to_csv(\"data/train_distance_data.csv\")\n",
    "val_with_distance.to_csv(\"data/val_distance_data.csv\")\n",
    "print(\"Calculating distance finished\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_with_distance.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}