{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycountry in /home/droman/Documents/stuff/dl_course/venv/lib/python3.8/site-packages (20.7.3)\r\n",
      "\u001B[33mWARNING: You are using pip version 20.2.3; however, version 20.3.1 is available.\r\n",
      "You should consider upgrading via the '/home/droman/Documents/stuff/dl_course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: langdetect in /home/droman/Documents/stuff/dl_course/venv/lib/python3.8/site-packages (1.0.8)\r\n",
      "Requirement already satisfied: six in /home/droman/Documents/stuff/dl_course/venv/lib/python3.8/site-packages (from langdetect) (1.15.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 20.2.3; however, version 20.3.1 is available.\r\n",
      "You should consider upgrading via the '/home/droman/Documents/stuff/dl_course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: transliterate in /home/droman/Documents/stuff/dl_course/venv/lib/python3.8/site-packages (1.10.2)\r\n",
      "Requirement already satisfied: six>=1.1.0 in /home/droman/Documents/stuff/dl_course/venv/lib/python3.8/site-packages (from transliterate) (1.15.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 20.2.3; however, version 20.3.1 is available.\r\n",
      "You should consider upgrading via the '/home/droman/Documents/stuff/dl_course/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pycountry\n",
    "!pip install langdetect\n",
    "!pip install transliterate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/droman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/droman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/droman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/droman/Documents/stuff/dl_course/venv/lib/python3.8/site-packages/tqdm/std.py:670: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "%pylab inline\n",
    "plt.style.use(\"bmh\")\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from functools import reduce\n",
    "\n",
    "import pycountry\n",
    "import re\n",
    "\n",
    "import langdetect\n",
    "from transliterate import translit, get_available_language_codes, slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train_converted.csv\")[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def get_popular_words(data):\n",
    "    documents = pd.concat([data['name_1'], data['name_2']]).values.astype('U')\n",
    "    count_vectorizer = CountVectorizer(max_features=1000) # memory limit: ngram_range=(2, 5),\n",
    "    values = count_vectorizer.fit_transform(documents)\n",
    "    frequency = values.toarray().sum(axis=0)\n",
    "\n",
    "    feature_names = count_vectorizer.get_feature_names()\n",
    "    count_data = pd.DataFrame({'feature_names': feature_names, 'frequency': frequency})\n",
    "\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000) # memory limit: ngram_range=(2, 5),\n",
    "    values = tfidf_vectorizer.fit_transform(documents)\n",
    "    frequency = values.toarray().sum(axis=0)\n",
    "\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    tf_idf_data = pd.DataFrame({'feature_names': feature_names, 'frequency': frequency})\n",
    "\n",
    "\n",
    "    tf_idf_data = tf_idf_data.sort_values(by=['frequency'], ascending=False)\n",
    "    count_data = count_data.sort_values(by=['frequency'], ascending=False)\n",
    "\n",
    "    perc = 0.2\n",
    "    size = int(len(count_data) * perc)\n",
    "\n",
    "    count_stop = count_data['feature_names'].iloc[:size].values\n",
    "    tf_idf_stop = tf_idf_data['feature_names'].iloc[:size].values\n",
    "\n",
    "    popular_words = set(count_stop) | set(tf_idf_stop)\n",
    "    return popular_words\n",
    "\n",
    "def transliterate_text(text):\n",
    "  t = slugify(text)\n",
    "  return t if t is not None else text\n",
    "\n",
    "def clear_data(data):\n",
    "    # cities = pd.read_csv(\"../cities.csv\", header=None, encoding='iso-8859-1')\n",
    "    stop_cities = []\n",
    "    # companies = pd.read_csv(\"../2020-11-19_elf-code-list-v1.3.csv\")\n",
    "    # duplicate_companies = []\n",
    "    # for i, (name_1, name_2) in companies.iterrows():\n",
    "    #     if isinstance(name_1, str) and isinstance(name_2, str):\n",
    "    #         if name_1 != name_2:\n",
    "    #             duplicate_companies.append((name_1, name_2))\n",
    "\n",
    "    data['name_1'] = data['name_1'].apply(transliterate_text)\n",
    "    data['name_2'] = data['name_2'].apply(transliterate_text)\n",
    "\n",
    "    countries = [country.name.lower() for country in pycountry.countries]\n",
    "\n",
    "    data[\"name_1\"] = data[\"name_1\"].str.lower()\n",
    "    data[\"name_2\"] = data[\"name_2\"].str.lower()\n",
    "\n",
    "    # REMOVE COMPANIES LABELS\n",
    "    legal_entities = [\"ltd.\", \"co.\", \"inc.\", \"b.v.\", \"s.c.r.l.\", \"gmbh\", \"pvt.\"]\n",
    "\n",
    "    for entity in tqdm(legal_entities):\n",
    "        data.replace(re.compile(f\"\\s+{entity}\\s*\"), \"\", inplace=True)\n",
    "\n",
    "    # REMOVE DATA IN CORNERS\n",
    "    data.replace(re.compile(r\"\\s+\\(.*\\)\"), \"\", inplace=True)\n",
    "    data.replace(re.compile(r\"[^\\w\\s]\"), \"\", inplace=True)\n",
    "\n",
    "    # REMOVE COUNTRY WORDS\n",
    "    for country in tqdm(countries):\n",
    "        data.replace(re.compile(country), \"\", inplace=True)\n",
    "\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "\n",
    "    data['name_1'] = data['name_1'].apply(lambda sentence: ' '.join([w for w in sentence.rstrip().split() if not w in stop_words]))\n",
    "    data['name_2'] = data['name_2'].apply(lambda sentence: ' '.join([w for w in sentence.rstrip().split() if not w in stop_words]))\n",
    "\n",
    "    # LEMMATIZE\n",
    "    # приводит к нормальной форме\n",
    "    lemmatizer = WordNetLemmatizer() #  lemmatizer.lemmatize(word)\n",
    "    # отрезает окончание\n",
    "    stemmer = PorterStemmer() # stemmer.stem(word)\n",
    "\n",
    "    data['name_1'] = data['name_1'].apply(lambda sentence: ' '.join([lemmatizer.lemmatize(w) for w in sentence.rstrip().split()]))\n",
    "    data['name_2'] = data['name_2'].apply(lambda sentence: ' '.join([lemmatizer.lemmatize(w) for w in sentence.rstrip().split()]))\n",
    "\n",
    "    data.fillna('', inplace=True)\n",
    "    popular_words = get_popular_words(data)\n",
    "\n",
    "    data['name_1'] = data['name_1'].apply(lambda s: ' '.join([w for w in s.rstrip().split() if not w in popular_words]))\n",
    "    data['name_2'] = data['name_2'].apply(lambda s: ' '.join([w for w in s.rstrip().split() if not w in popular_words]))\n",
    "\n",
    "    data = data.drop_duplicates()\n",
    "    data = data.dropna()\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 1300.73it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 1671.43it/s]\n"
     ]
    }
   ],
   "source": [
    "data = clear_data(train)\n",
    "data.to_csv(\"test_clear_data.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_extra_relations(data):\n",
    "    data = data[data.is_duplicate==1]\n",
    "    data = data[['name_1','name_2']]\n",
    "\n",
    "    def get_same_companies_graph(companies_dataframe):\n",
    "        result = [] # [[[a,b,c], {a: [b, c], b: [c]}, 3], [[d,e], {d: [e]}], 2]\n",
    "        for i, pair in tqdm(companies_dataframe.iterrows()):\n",
    "            first = pair['name_1']\n",
    "            second = pair['name_2']\n",
    "            if first != second:\n",
    "                flag = True\n",
    "\n",
    "                for list_same_companies in result:\n",
    "                    if first in list_same_companies[0]:\n",
    "                        if second not in list_same_companies[0]:\n",
    "                            list_same_companies[0].append(second)\n",
    "                            try:\n",
    "                                list_same_companies[1][first].append(second)\n",
    "                            except:\n",
    "                                list_same_companies[1].update({first: [second]})\n",
    "                            list_same_companies[2]+=1\n",
    "                            flag = False\n",
    "                    else:\n",
    "                        if second in list_same_companies[0]:\n",
    "                            if first not in list_same_companies[0]:\n",
    "                                list_same_companies[0].append(first)\n",
    "                                try:\n",
    "                                    list_same_companies[1][second].append(first)\n",
    "                                except:\n",
    "                                    list_same_companies[1].update({second: [first]})\n",
    "                                list_same_companies[2]+=1\n",
    "                                flag = False\n",
    "                if flag:\n",
    "                    result.append([[first, second], {first: [second]}, 1])\n",
    "        return result\n",
    "\n",
    "    companies_graph = get_same_companies_graph(data)\n",
    "\n",
    "    LIST_UNIQUE_COMPANIES_ID = 0\n",
    "    DICT_OF_RELATIONS_ID = 1\n",
    "    COUNT_OF_RELATIONS_ID = 2\n",
    "    EXTRA_RELATIONS_ID = 3\n",
    "    def get_extra_relations(inner_companies_graph):\n",
    "        extra_relations = []\n",
    "        for unique_companies in tqdm(inner_companies_graph):\n",
    "            unique_companies.append([])\n",
    "            companies = unique_companies[LIST_UNIQUE_COMPANIES_ID]\n",
    "            company_relations = unique_companies[DICT_OF_RELATIONS_ID]\n",
    "            for company in companies:\n",
    "                try:\n",
    "                    relations = company_relations[company]\n",
    "                    excluded_companies = relations\n",
    "                except:\n",
    "                    excluded_companies = []\n",
    "                excluded_companies.append(company)\n",
    "                for extra_relation_company in companies:\n",
    "                    if extra_relation_company not in excluded_companies:\n",
    "                        extra_relations.append([company, extra_relation_company])\n",
    "                        unique_companies[COUNT_OF_RELATIONS_ID]+=1\n",
    "                        unique_companies[EXTRA_RELATIONS_ID].append([company, extra_relation_company])\n",
    "        return extra_relations\n",
    "\n",
    "    extra_relations = get_extra_relations(companies_graph)\n",
    "    df = pd.DataFrame(extra_relations, columns = ['name_1','name_2'])\n",
    "    df['is_duplicate'] =   1\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_abbreviation(data):\n",
    "    data = data[data.is_duplicate==0]\n",
    "\n",
    "    data = data['name_1'].append(data['name_2']).reset_index(drop=True)\n",
    "    def filter_condition(sentence):\n",
    "        words = sentence.split()\n",
    "        return 2 < len(words) < 6 and all(len(word)>3 for word in words)\n",
    "\n",
    "    data = list(filter(filter_condition, data.tolist()))\n",
    "\n",
    "    def _get_abbreviation(sentence):\n",
    "        return ''.join([word[0] for word in sentence.split()])\n",
    "    data = [[sentence, _get_abbreviation(sentence)] for sentence in data]\n",
    "\n",
    "    return pd.DataFrame(data, columns=['name_1', 'name_2'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}